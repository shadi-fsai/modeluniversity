teacher_role:  #change tax preparers to what you're looking to train
  You are a teacher of tax preparers.

student_role:  #change tax preparers to what you're looking to train
  You are a tax preparer.

curriculum_prompt:  #change tax topics/tax preparer to what you're looking to train
  Create a very thorough list of tax topics a tax preparer must understand to be a good tax preparer. For each topic create a very thorough list of subtopics that a tax preparer must understand. Do not omit anything.

num_practice_questions: 15
num_easy_practice_questions: 5
num_medium_practice_questions: 5
num_hard_practice_questions: 5
practice_allow_expansion: true # allow adding more questions to ensure covering core concepts

num_test_questions: 5
num_easy_test_questions: 1
num_medium_test_questions: 2
num_hard_test_questions: 2
test_allow_expansion: true # allow adding more questions to ensure covering core concepts

datagen_model: groq/Llama-3.3-70b-Versatile # recommend using a capable model here

llm_evals_list: # list of models to evaluate the tests on; using litellm model strings
#  - "bedrock/us.meta.llama3-2-1b-instruct-v1:0"
  - "ollama/hf.co/shadicopty/Llama3.2-1b-taxadvisor"
#  - "groq/Llama-3.3-70b-Versatile"
#  - "gemini/gemini-1.5-flash"
#  - "bedrock/us.meta.llama3-2-3b-instruct-v1:0"
#  - "groq/llama-3.2-3b-preview"
#  - "ollama/hf.co/shadicopty/Llama3.2-3b-taxadvisor"
#  - "gpt-4o"
#  - "gpt-4o-mini"


opik_dataset:  #change to the dataset you're looking to use
  Tax Advisor Dataset

opik_eval_model: gpt-4o-mini   # any cheap model that handles json would do

closed_textbook_eval: false  # evaluate the models without access to the textbook
open_textbook_eval: true  # evaluate the models with access to the textbook